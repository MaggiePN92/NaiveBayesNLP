{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Notice: there is a pickle dump available.\n",
    "\n",
    "#downloads:  \n",
    "#!pip install regex\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download(\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import math\n",
    "import nltk\n",
    "import regex as re\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.util import mark_negation\n",
    "from math import log\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/thanhtut/info284_lab/master/assignment1/twitter-airline-sentiment/Tweets.csv\"\n",
    "\n",
    "#Creating dataset of csv-file from url.\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "#Create dataset with airline sentiment and text (notice I'm only using text as a feature).\n",
    "dataset = df[[\"airline_sentiment\",\"text\"]]\n",
    "\n",
    "#Pickle file with model data. Contains: [vocabulary, logprior, loglike]\n",
    "#read_ml = open(\"ml-oblig1\",\"rb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ex 1: cleaning and splitting of data\n",
    "\n",
    "def data_cleaner(dataframe):\n",
    "    #unicodes for different emojis\n",
    "    RE_EMOJI = re.compile('[\\U00010000-\\U0010ffff]', flags=re.UNICODE) \n",
    "    #stopwords\n",
    "    stopwords_helpset = set(stopwords.words(\"english\"))\n",
    "    #stemmer \n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    #list for clean data\n",
    "    clean_data = []\n",
    "    \n",
    "    #iterates the data\n",
    "    for index, row in dataframe.iterrows():\n",
    "        #all words in lowercase and bigger than or equal to 3\n",
    "        filtered_word = [word.lower() for word in row.text.split() if len(word) >= 3]\n",
    "        #words without @, http and is not RT. Word also has to start with a character\n",
    "        cleaned_word = [word for word in filtered_word \\\n",
    "                        if word != \"RT\"\\\n",
    "                        and \"@\" not in word\\\n",
    "                        and \"#\" not in word\\\n",
    "                        and \"http\" not in word\\\n",
    "                        and word[0].isalpha()\\\n",
    "                        ]\n",
    "\n",
    "        #list of words without emojis\n",
    "        remove_smiley = [RE_EMOJI.sub(r'', word) for word in cleaned_word]\n",
    "        #list of words with negation mark\n",
    "        mark_neg = mark_negation(remove_smiley)\n",
    "        #remove stopwords with nltk\n",
    "        non_stopwords = [word for word in mark_neg if not word in stopwords_helpset]\n",
    "        #stemming words\n",
    "        stemmed_list = [stemmer.stem(word) for word in non_stopwords]\n",
    "        #removing punctuation\n",
    "        non_punct = [word.translate(str.maketrans('', '', string.punctuation)) for word in stemmed_list]\n",
    "        #clean data:\n",
    "        clean_data.append((non_punct, row.airline_sentiment))\n",
    "        \n",
    "        \n",
    "    return clean_data \n",
    "\n",
    "\n",
    "def clean_frame(dataframe, set_test_size = 0.1):\n",
    "    clean_list = data_cleaner(dataframe)\n",
    "    clean_dataset = pd.DataFrame(clean_list, columns = [\"text\",\"sentiment\"])\n",
    "    \n",
    "    #split dataset into training and testing, test is 10% of dataset\n",
    "    data_train, data_test = train_test_split(clean_dataset, test_size = set_test_size)\n",
    "    return data_train, data_test\n",
    "\n",
    "\n",
    "data_train, data_test = clean_frame(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ex 2 (vocabularies) and 8(Laplace smoothing):\n",
    "def word_counter2(data_train):\n",
    "    \n",
    "    #All positive, negative and then neutral words in dataset, will use this list for counting.\n",
    "    flat_pos = [val for sublist in\\\n",
    "                  data_train[data_train[\"sentiment\"]\\\n",
    "                             ==\"positive\"][\"text\"].tolist()\\\n",
    "                  for val in sublist]\n",
    "\n",
    "    flat_neg = [val for sublist in\\\n",
    "                      data_train[data_train[\"sentiment\"]\\\n",
    "                                 ==\"negative\"][\"text\"].tolist()\\\n",
    "                      for val in sublist]\n",
    "\n",
    "    flat_neu = [val for sublist in\\\n",
    "                      data_train[data_train[\"sentiment\"]\\\n",
    "                                 ==\"neutral\"][\"text\"].tolist()\\\n",
    "                      for val in sublist]\n",
    "    \n",
    "    #vocabulary\n",
    "    vocabulary = [val for sublist in\\\n",
    "                      data_train[\"text\"].tolist()\\\n",
    "                      for val in sublist]\n",
    "    \n",
    "    #Dictonaries will have to form of i.e.: pos_dict[\"late\"] = 2,\n",
    "    #where \"late\" is the word and 2 is the number of occourance of word in flat_pos.\n",
    "    #Also notice that I'm using Laplace smoothing\n",
    "    pos_dict = dict(zip((set(vocabulary)), [flat_pos.count(word) + 1 for word in set(vocabulary)]))\n",
    "    pos_dict[\"total_word\"] = sum(value for value in pos_dict.values())\n",
    "    \n",
    "    neg_dict = dict(zip((set(vocabulary)), [flat_neg.count(word) + 1 for word in set(vocabulary)]))\n",
    "    neg_dict[\"total_word\"] = sum(value for value in neg_dict.values())\n",
    "    \n",
    "    neu_dict = dict(zip((set(vocabulary)), [flat_neu.count(word) + 1 for word in set(vocabulary)]))\n",
    "    neu_dict[\"total_word\"] = sum(value for value in neu_dict.values())\n",
    "        \n",
    "       \n",
    "    return (vocabulary, pos_dict, neg_dict, neu_dict)\n",
    "\n",
    "vocabulary, pos_dict, neg_dict, neu_dict = word_counter2(data_train)\n",
    "\n",
    "#Ex 3: I do not use metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ex 4 logprior:\n",
    "\n",
    "def logprior(data_train):\n",
    "    \n",
    "    N_doc = len(data_train) #Number of docs in training dataset.\n",
    "    N_c = data_train[\"sentiment\"].value_counts() #Number of docs for each sentiment.\n",
    "    \n",
    "    #Making logpriors\n",
    "    logprior = {}\n",
    "    logprior[\"positive\"]= math.log10(N_c[\"positive\"]/N_doc)\n",
    "    logprior[\"negative\"]= math.log10(N_c[\"negative\"]/N_doc)\n",
    "    logprior[\"neutral\"]= math.log10(N_c[\"neutral\"]/N_doc)\n",
    "    \n",
    "    return logprior\n",
    "\n",
    "logprior = logprior(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ex 5 loglikelihood:\n",
    "\n",
    "def create_loglike(pos_dict,neg_dict,neu_dict):\n",
    "    #I append the calculated loglikelihoods for each word to the list loglike.\n",
    "    loglike = [] \n",
    "    for word in set(vocabulary):\n",
    "        pos = math.log10((pos_dict[word])/(pos_dict[\"total_word\"]))\n",
    "        neg = math.log10((neg_dict[word])/(neg_dict[\"total_word\"]))\n",
    "        neu = math.log10((neu_dict[word])/(neu_dict[\"total_word\"]))\n",
    "        loglike.append((word,pos,neg,neu))\n",
    "    \n",
    "    return loglike\n",
    "\n",
    "loglike = create_loglike(pos_dict,neg_dict,neu_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ex 6 naive bayes prediction model:\n",
    "\n",
    "def pred_nb(text, logprior, loglike, vocabulary):\n",
    "    summ = logprior.copy()\n",
    "    pred_classes = [\"positive\", \"negative\", \"neutral\"]\n",
    "    \n",
    "    for word in text:\n",
    "        if word in set(vocabulary):\n",
    "            value_list = [x for x in loglike if x[0] == word]\n",
    "            \n",
    "            i = 1\n",
    "            for pred in pred_classes:\n",
    "                summ[pred] += summ[pred] + value_list[0][i]\n",
    "                i += 1\n",
    "    \n",
    "    maxi = max(summ[\"positive\"],summ[\"negative\"],summ[\"neutral\"])\n",
    "    \n",
    "    if maxi == summ[\"neutral\"]:\n",
    "        return \"neutral\"\n",
    "        \n",
    "    elif maxi == summ[\"positive\"]:\n",
    "        return \"positive\"\n",
    "    \n",
    "    else:\n",
    "        return \"negative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6584699453551912"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ex 7 evaluating the model:\n",
    "\n",
    "def evaluate_nb(data_test, logprior, loglike, vocabulary):\n",
    "    correct = 0\n",
    "    total = len(data_test)\n",
    "\n",
    "\n",
    "    for index, row in data_test.iterrows():\n",
    "        result = pred_nb(row.text,logprior, loglike, vocabulary)\n",
    "\n",
    "        if result==row.sentiment:\n",
    "            correct += 1\n",
    "\n",
    "    pred_rate = correct/total\n",
    "    return pred_rate\n",
    "\n",
    "evaluate_nb(data_test, logprior, loglike, vocabulary)\n",
    "\n",
    "#result: 0.68"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 39 182   2]\n",
      " [  2 907   6]\n",
      " [  6 304  16]]\n"
     ]
    }
   ],
   "source": [
    "def confusion_matrix(data_test, logprior, loglike, vocabulary):\n",
    "    \n",
    "    pos_result = {\"pos\":0,\"neg\":0,\"neu\":0}\n",
    "    neg_result = {\"pos\":0,\"neg\":0,\"neu\":0}\n",
    "    neu_result = {\"pos\":0,\"neg\":0,\"neu\":0}\n",
    "    keys = [\"pos\",\"neg\",\"neu\"]\n",
    "    \n",
    "    for index, row in data_test.iterrows():\n",
    "        result = pred_nb(row.text,logprior, loglike, vocabulary)\n",
    "        \n",
    "        if row.sentiment == \"positive\":\n",
    "            if result == \"positive\": pos_result[\"pos\"] += 1\n",
    "            elif result == \"negative\": pos_result[\"neg\"] += 1\n",
    "            elif result == \"neutral\": pos_result[\"neu\"] += 1\n",
    "                \n",
    "        elif row.sentiment == \"negative\":\n",
    "            if result == \"positive\": neg_result[\"pos\"] += 1\n",
    "            elif result == \"negative\": neg_result[\"neg\"] += 1\n",
    "            elif result == \"neutral\": neg_result[\"neu\"] += 1\n",
    "                \n",
    "        elif row.sentiment == \"neutral\":\n",
    "            if result == \"positive\": neu_result[\"pos\"] += 1\n",
    "            elif result == \"negative\": neu_result[\"neg\"] += 1\n",
    "            elif result == \"neutral\": neu_result[\"neu\"] += 1\n",
    "                \n",
    "\n",
    "    conf_matrix = np.matrix([[pos_result[i] for i in keys],\\\n",
    "                         [neg_result[i] for i in keys],\\\n",
    "                         [neu_result[i] for i in keys]])\n",
    "    \n",
    "    return conf_matrix\n",
    "\n",
    "    \n",
    "conf_mat = confusion_matrix(data_test, logprior, loglike, vocabulary)\n",
    "\n",
    "#[[ 39 182   2]\n",
    "# [  2 907   6]\n",
    "# [  6 304  16]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ex 10 explanation:\n",
    "\n",
    "def explanation_nb(text, logprior, loglike, vocabulary):\n",
    "    pred_classes = [\"positive\", \"negative\", \"neutral\"]\n",
    "    summ = logprior.copy()\n",
    "\n",
    "    expl = {pred: [] for pred in pred_classes}\n",
    "    for pred in pred_classes:\n",
    "        expl[pred].append((\"logprior:\",summ[pred]))\n",
    "\n",
    "    for word in text:\n",
    "        if word in set(vocabulary):\n",
    "            value_list = [x for x in loglike if x[0] == word]\n",
    "            \n",
    "            tup_word = str(word)+\":\"\n",
    "            i = 1\n",
    "            for pred in pred_classes:\n",
    "                expl[pred].append((tup_word,value_list[0][i]))\n",
    "                i += 1\n",
    "                \n",
    "    pred = pred_nb(text,logprior, loglike, vocabulary)\n",
    "    \n",
    "    print(\"Prediction:\", pred)\n",
    "    print(\"List of words used in analysis after cleaning: \", text)\n",
    "    \n",
    "    for pred_class in pred_classes: \n",
    "        print(\"-----------------------------------------\")\n",
    "        print(pred_class.capitalize()+\" score: {}\".format(round(summ[pred_class],2)))\n",
    "        print(\"Explanation:\")\n",
    "        \n",
    "        for tup in expl[pred_class]:\n",
    "            print(str(tup[0]),round(tup[1],3))\n",
    "        if pred_class == \"neutral\":\n",
    "            print(\"-----------------------------------------\")\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model data loaded.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    884\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2abfca448bb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mmath\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0mcommand_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-2abfca448bb4>\u001b[0m in \u001b[0;36mcommand_line\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mloglike\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave_list1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mexpl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Press y if you want an explanation, if else press enter.\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mexpl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter to quit in following sequence.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    861\u001b[0m         )\n\u001b[1;32m    862\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    888\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 890\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    891\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Ex 9 command line:\n",
    "\n",
    "def commandline_test_nb(text, expl = None):\n",
    "    text = text.split()\n",
    "    filtered_word = [word.lower() for word in text if len(word) >= 3]\n",
    "    #words without @, http and is not RT. Word also has to start with a character\n",
    "    cleaned_word = [word for word in filtered_word \\\n",
    "                    if \"@\" not in word\\\n",
    "                    and \"#\" not in word\\\n",
    "                    and word != \"RT\"\\\n",
    "                    and word[0].isalpha()\\\n",
    "                    and \"http\" not in word\\\n",
    "                    ]\n",
    "    \n",
    "    #list of words without emojis\n",
    "    remove_smiley = [RE_EMOJI.sub(r'', word) for word in cleaned_word]\n",
    "    #list of words with negation mark\n",
    "    mark_neg = mark_negation(remove_smiley)\n",
    "    #remove stopwords with nltk\n",
    "    non_stopwords = [word for word in mark_neg if not word in stopwords_helpset]\n",
    "    #stemming words\n",
    "    stemmed_list = [stemmer.stem(word) for word in non_stopwords]\n",
    "    #removing punctuation\n",
    "    pred_list = [word.translate(str.maketrans('', '', string.punctuation)) for word in stemmed_list]\n",
    "    \n",
    "    if expl == \"y\": \n",
    "        result = explanation_nb(pred_list,logprior, loglike, vocabulary)\n",
    "        return result\n",
    "    else: \n",
    "        result = pred_nb(pred_list,logprior, loglike, vocabulary)\n",
    "        return result\n",
    "\n",
    "def command_line():\n",
    "    \n",
    "    #Pickle dump.\n",
    "    read_ml = open(\"ml-oblig1\",\"rb\")\n",
    "    #Contains: [vocabulary, logprior, loglike].\n",
    "    try:\n",
    "        save_list1 = pickle.load(read_ml)\n",
    "        print(\"Model data loaded.\")\n",
    "    except:\n",
    "        print(\"Failed to load model data.\")\n",
    "        print(\"Make sure data dump is in correct directory.\")\n",
    "    \n",
    "    vocabulary=save_list1[0]\n",
    "    logprior=save_list1[1]\n",
    "    loglike=save_list1[2]\n",
    "    \n",
    "    expl = str(input(\"Press y if you want an explanation, if else press enter.\\n\"))\n",
    "    expl = expl.lower()\n",
    "    print(\"Enter to quit in following sequence.\")\n",
    "    \n",
    "    while True:\n",
    "        tweet = str(input(\"Tweet to analyse: \\n\"))\n",
    "        print(\"*****************************************\")\n",
    "        \n",
    "        if tweet == \"\":\n",
    "            #save_ml = open(\"ml-oblig1\",\"ab\")\n",
    "            #save_list = [vocabulary, logprior, loglike]\n",
    "            #pickle.dump(save_list,\"ml-oblig1\")\n",
    "            print(\"Terminating\")\n",
    "            \n",
    "            break\n",
    "            \n",
    "        \n",
    "        if expl == \"y\": result = commandline_test_nb(tweet, \"y\")\n",
    "        else: \n",
    "            result = commandline_test_nb(tweet)\n",
    "            print(\"Predicted sentiment: \",result)\n",
    "        print(\"*****************************************\")\n",
    "        \n",
    "   \n",
    "if __name__ == '__main__':\n",
    "    import pickle\n",
    "    import pandas as pd\n",
    "    import string\n",
    "    import math\n",
    "    import nltk\n",
    "    import regex as re\n",
    "    \n",
    "    from nltk import sent_tokenize, word_tokenize\n",
    "    from nltk.stem.snowball import SnowballStemmer\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.sentiment.util import mark_negation\n",
    "    from math import log \n",
    "\n",
    "    command_line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ex 11:\n",
    "\n",
    "#Confusion matrix =\n",
    "#      pos neg  neu\n",
    "#[pos[ 39  182   2]\n",
    "# neg[  2  907   6]\n",
    "# neu[  6  304  16]]\n",
    "\n",
    "#accuracy = 0.68\n",
    "\n",
    "#The main problem of this model arises from the huge imbalance in the dataset. Most tweets are negative, this\n",
    "#means that logprior most of the times will outweigh loglikelihood. The effect of this is a model that mostly\n",
    "#predicts negative tweets. An accuracy of 68 is a little better than if the model would predict negative\n",
    "#for all inputs. \n",
    "#The tweets the model predict correctly are longer positive tweets where the number of positive words\n",
    "#(loglikelihood) outweigh negative to positive ratio (logprior) in the dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
